{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk.corpus import stopwords\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv_column_arr(data_frame, field, field_arr):\n",
    "    list_f_arr = []\n",
    "    dict_hasil = {}\n",
    "    f_arr = data_frame[field_arr]\n",
    "    for i in f_arr:\n",
    "        deleted_simbol = i.replace(\" \",\"\").replace(\"'\", \"\").replace(\"[\",\"\").replace(\"]\",\"\")\n",
    "        temp = deleted_simbol.split(\",\")\n",
    "        \n",
    "#         dict_hasil[term[i]] = num_doc\n",
    "        list_f_arr.append(temp)\n",
    "    dict_hasil = {field: data_frame[field], field_arr:list_f_arr }\n",
    "    df = pd.DataFrame(dict_hasil)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Baca File CSV yang Di Perlukan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "   doc_num                                              token\n0        1  [0.39, 1, 750, 1, 780, 1, 850, 1, 870, 1, 875,...\n1        2  [15:02:20.00, 26-feb-1987, 55, activ, also, am...\n2        3  [13.5, 15:03:27.51, 26-feb-1987, 31, 7.5, appl...\n3        4  [-1, -20, 1, 11, 12, 15, 15:07:13.72, 2, 2.70,...\n4        5  [-1986, 0.99, 1, 1.24, 1.35, 1.56, 1.65, 1.92,...",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>doc_num</th>\n      <th>token</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1</td>\n      <td>[0.39, 1, 750, 1, 780, 1, 850, 1, 870, 1, 875,...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2</td>\n      <td>[15:02:20.00, 26-feb-1987, 55, activ, also, am...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>3</td>\n      <td>[13.5, 15:03:27.51, 26-feb-1987, 31, 7.5, appl...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>4</td>\n      <td>[-1, -20, 1, 11, 12, 15, 15:07:13.72, 2, 2.70,...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>5</td>\n      <td>[-1986, 0.99, 1, 1.24, 1.35, 1.56, 1.65, 1.92,...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 3
    }
   ],
   "source": [
    "df_doc_num_token = pd.read_csv(\"doc_num_token.csv\")\n",
    "df_doc_num_token = conv_column_arr(df_doc_num_token,\"doc_num\",\"token\")\n",
    "df_doc_num_token.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "   term                                            doc_num\n0    -1  [0004, 0008, 0016, 0026, 0059, 0059, 0059, 005...\n1   -10                                             [0305]\n2  -100                                             [0427]\n3   -11                                             [0116]\n4  -110                                             [0427]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>term</th>\n      <th>doc_num</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>-1</td>\n      <td>[0004, 0008, 0016, 0026, 0059, 0059, 0059, 005...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>-10</td>\n      <td>[0305]</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>-100</td>\n      <td>[0427]</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>-11</td>\n      <td>[0116]</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>-110</td>\n      <td>[0427]</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 4
    }
   ],
   "source": [
    "df_token_doc_num = pd.read_csv(\"hasil_preprocessing.csv\")\n",
    "df_token_doc_num = conv_column_arr(df_token_doc_num, \"term\", \"doc_num\")\n",
    "df_token_doc_num.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "49088"
     },
     "metadata": {},
     "execution_count": 5
    }
   ],
   "source": [
    "# menghitung jml token\n",
    "jml_token = 0\n",
    "for i in range(len(df_doc_num_token)):\n",
    "    jml_token += len(df_doc_num_token[\"token\"].iloc[i])\n",
    "jml_token"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fungsi-fungsi untuk preprocessing query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_token(txt):\n",
    "    list_hasil = txt.split(\" \")            \n",
    "    return list_hasil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def removeNull(listOfWords): #['','','eat','food','','']\n",
    "    listOfWords = list(filter(None, listOfWords))\n",
    "    \n",
    "    return listOfWords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def caseFolding(listOfWords):\n",
    "    for i in range(len(listOfWords)):\n",
    "        listOfWords[i] = listOfWords[i].casefold()\n",
    "        \n",
    "    return listOfWords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def removeStopWord(listOfWords):\n",
    "    for i in listOfWords:\n",
    "        if i in stop_words:\n",
    "            listOfWords.remove(i)\n",
    "        \n",
    "    return listOfWords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stemming(listOfWords):\n",
    "    stemmer = SnowballStemmer(\"english\")\n",
    "    stemmed = [stemmer.stem(word) for word in listOfWords]\n",
    "    \n",
    "    return stemmed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fungsi utama main preprocessing query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pre_query(txt):\n",
    "    listOfWords = make_token(txt)\n",
    "    listOfWords = removeNull(listOfWords)\n",
    "    listOfWords = caseFolding(listOfWords)\n",
    "    listOfWords = removeStopWord(listOfWords)\n",
    "    listOfWords = stemming(listOfWords)\n",
    "    \n",
    "    return listOfWords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fungsi utama menghitung Score dokumen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hitung_score(list_word,a):\n",
    "    list_rank = []\n",
    "    list_doc_num2 = []\n",
    "    df_hasil = []\n",
    "    for i in range(len(df_doc_num_token)):\n",
    "        rank = 1\n",
    "        doc = df_doc_num_token.iloc[i]\n",
    "        for j in list_word:\n",
    "            count = doc[\"token\"].count(j)\n",
    "            panjang_doc = len(doc[\"token\"])\n",
    "            p_doc =  (count/panjang_doc) * a\n",
    "#           ----------------------------------\n",
    "            temp = df_token_doc_num[df_token_doc_num[\"term\"] == j]\n",
    "#             print(i, temp)\n",
    "            count_m = len(temp.iloc[0][1])\n",
    "            p_m = (count_m/jml_token) * (1-a)\n",
    "            p = p_doc + p_m\n",
    "            rank *= p\n",
    "        list_rank.append(rank)\n",
    "        list_doc_num2.append(doc[\"doc_num\"])\n",
    "    dict_hasil = {\"doc_num\": list_doc_num2, \"rank\": list_rank}\n",
    "    df_hasil = pd.DataFrame(dict_hasil)\n",
    "    df_hasil = df_hasil.sort_values(by=[\"rank\"], ignore_index=True, ascending=False)\n",
    "    return df_hasil\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Coba query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "q1 = \"1991\"\n",
    "q2 = \"Discounts\"\n",
    "q3 = \"accessed 9-Mar-87\"\n",
    "q4 = \"Analyst appeared\"\n",
    "q5 = \"Analyst analyst appeared\"\n",
    "q6 = \"Chairman closed the last\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "'discount'"
     },
     "metadata": {},
     "execution_count": 15
    }
   ],
   "source": [
    "q = pre_query(q2)\n",
    "search2 = ' '.join(map(str, q))\n",
    "search2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_hasil = hitung_score(q,0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_jawaban = {\n",
    "    \"1991\" : [31, 61, 70, 105, 190, 301, 422, 466],\n",
    "    \"discount\" : [77, 80, 97, 160, 195, 208, 215, 256, 488],\n",
    "    \"access 9-Mar-87\" : [32, 55, 110, 116, 134, 141, 142, 143, 144, 145, 146, 147, 148,\n",
    "                149, 150, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 261, \n",
    "                262, 263, 264, 265, 266, 267, 268, 269, 270, 321, 322, 323, 324, \n",
    "                325, 326, 327, 328, 329, 330, 348, 375, 381, 382, 383, 384, 385, \n",
    "                386, 387, 388, 389, 390, 441, 442, 443, 444, 445, 446, 447, 448,\n",
    "                449, 450],\n",
    "    \"analyst appear\" : [4, 8, 16, 18, 28, 45, 78, 95, 98, 111, 123, 133, 150, 180, 195, \n",
    "                235, 243, 262, 265, 292, 305, 320, 376, 399, 418, 459, 469, 493, \n",
    "                495, 496, 497],\n",
    "    \"analyst analyst appear\" : [4, 8, 16, 18, 28, 45, 78, 95, 98, 111, 123, 133, 150, 180, 195, \n",
    "                235, 243, 262, 265, 292, 305, 320, 376, 399, 418, 459, 469, 493, \n",
    "                495, 496, 497],\n",
    "    \"chairman close last\" : [1, 12, 18, 26, 28, 40, 43, 47, 52, 60, 61, 69, 78, 95, 98, 102, \n",
    "            107, 111, 114, 116, 133, 134, 135, 136, 138, 145, 148, 150, 153, \n",
    "            154, 155, 156, 161, 169, 174, 178, 180, 188, 194, 195, 197, 201, \n",
    "            203, 204, 207, 208, 214, 216, 224, 227, 230, 231, 232, 233, 234, \n",
    "            235, 242, 246, 254, 255, 256, 259, 261, 262, 263, 265, 267, 268, \n",
    "            269, 276, 279, 284, 288, 292, 303, 306, 314, 316, 318, 322, 324, \n",
    "            328, 330, 338, 339, 340, 343, 350, 355, 367, 371, 372, 375, 376, \n",
    "            378, 382, 383, 386, 392, 399, 404, 405, 409, 419, 420, 425, 430, \n",
    "            438, 439, 441, 444, 445, 446, 447, 448, 451, 454, 455, 457, 458, \n",
    "            463, 484, 488, 491, 492, 494, 495, 496, 498]\n",
    "}               "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "{'1991': [31, 61, 70, 105, 190, 301, 422, 466], 'discount': [77, 80, 97, 160, 195, 208, 215, 256, 488], 'access 9-Mar-87': [32, 55, 110, 116, 134, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 261, 262, 263, 264, 265, 266, 267, 268, 269, 270, 321, 322, 323, 324, 325, 326, 327, 328, 329, 330, 348, 375, 381, 382, 383, 384, 385, 386, 387, 388, 389, 390, 441, 442, 443, 444, 445, 446, 447, 448, 449, 450], 'analyst appear': [4, 8, 16, 18, 28, 45, 78, 95, 98, 111, 123, 133, 150, 180, 195, 235, 243, 262, 265, 292, 305, 320, 376, 399, 418, 459, 469, 493, 495, 496, 497], 'analyst analyst appear': [4, 8, 16, 18, 28, 45, 78, 95, 98, 111, 123, 133, 150, 180, 195, 235, 243, 262, 265, 292, 305, 320, 376, 399, 418, 459, 469, 493, 495, 496, 497], 'chairman close last': [1, 12, 18, 26, 28, 40, 43, 47, 52, 60, 61, 69, 78, 95, 98, 102, 107, 111, 114, 116, 133, 134, 135, 136, 138, 145, 148, 150, 153, 154, 155, 156, 161, 169, 174, 178, 180, 188, 194, 195, 197, 201, 203, 204, 207, 208, 214, 216, 224, 227, 230, 231, 232, 233, 234, 235, 242, 246, 254, 255, 256, 259, 261, 262, 263, 265, 267, 268, 269, 276, 279, 284, 288, 292, 303, 306, 314, 316, 318, 322, 324, 328, 330, 338, 339, 340, 343, 350, 355, 367, 371, 372, 375, 376, 378, 382, 383, 386, 392, 399, 404, 405, 409, 419, 420, 425, 430, 438, 439, 441, 444, 445, 446, 447, 448, 451, 454, 455, 457, 458, 463, 484, 488, 491, 492, 494, 495, 496, 498]}\n"
    }
   ],
   "source": [
    "print(dict_jawaban)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "[77, 80, 97, 160, 195, 208, 215, 256, 488]"
     },
     "metadata": {},
     "execution_count": 19
    }
   ],
   "source": [
    "get = dict_jawaban.get(search2)\n",
    "get"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "NameError",
     "evalue": "name 'jawaban1' is not defined",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-20-fcf797ddd99c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mteratas\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdf_hasil\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mjawaban1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msort_values\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mby\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"doc_num\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mignore_index\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mascending\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'jawaban1' is not defined"
     ]
    }
   ],
   "source": [
    "teratas = df_hasil.head(len(jawaban1)).sort_values(by=[\"doc_num\"], ignore_index=True, ascending=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "output_type": "error",
     "ename": "NameError",
     "evalue": "name 'teratas' is not defined",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-21-dd75c0205713>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mhasil_query\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mteratas\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"doc_num\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0msalah\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mhasil_query\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mjawaban1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'teratas' is not defined"
     ]
    }
   ],
   "source": [
    "hasil_query = teratas[\"doc_num\"]\n",
    "salah = []\n",
    "\n",
    "for i in hasil_query:\n",
    "    if int(i) not in jawaban1:\n",
    "        salah.append(i)\n",
    "        \n",
    "len(salah)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "NameError",
     "evalue": "name 'teratas' is not defined",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-22-550e74260902>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mteratas\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'teratas' is not defined"
     ]
    }
   ],
   "source": [
    "teratas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from flask import Flask, request, render_template, url_for, redirect, jsonify"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_query(search, res):\n",
    "    res2 = int(res)\n",
    "    list_hasil = []\n",
    "    list_score = []\n",
    "    q = pre_query(search)\n",
    "    df_hasil = hitung_score(q,0.1)\n",
    "    for i in range(0, res2):\n",
    "        list_hasil.append(df_hasil.iloc[i][\"doc_num\"])\n",
    "        list_score.append(df_hasil.iloc[i][\"rank\"])\n",
    "    return list_hasil, list_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def readText(file):\n",
    "    words = []\n",
    "    f = open(file, 'r') #open file\n",
    "    text = f.read()    \n",
    "    f.close()\n",
    "\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_atribut(doc_id):\n",
    "    int_doc_id = int(doc_id)\n",
    "    str_doc_id = str(int_doc_id)\n",
    "    n = 4 - len(str_doc_id)\n",
    "    doc = (\"0\"*n) + str_doc_id\n",
    "    file_path = \"DataRouter\\\\Doc\" + doc + \".txt\"\n",
    "    \n",
    "    txt = readText(file_path)\n",
    "    \n",
    "    titlePattern = \"<TITLE>(.+)</TITLE>\"\n",
    "    title = re.findall(titlePattern, txt)[0]\n",
    "    \n",
    "    datePattern = \"<DATE>(.+)</DATE>\"\n",
    "    date = re.findall(datePattern, txt)[0]\n",
    "    \n",
    "    bodyPattern = \"<BODY>((.+\\s+)+)</BODY>\"\n",
    "    body = re.findall(bodyPattern, txt)[0]\n",
    "    print(body)\n",
    "    \n",
    "    return title, date, body"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>PUNYA BONTENG TF-IDF</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def searchQuery(_dir, _query):\n",
    "    file = open(_dir,'r')\n",
    "    \n",
    "    listQuery = pre_query(_query)\n",
    "    \n",
    "    dictOfQuery = {}\n",
    "    textPerLine = file.readlines()\n",
    "    for line in textPerLine:\n",
    "        temp = line.split('\\t')\n",
    "        if temp[0] in listQuery:\n",
    "            addLineToDict(dictOfQuery, line)\n",
    "            \n",
    "#     for i in dictOfQuery:\n",
    "#         print(i,' : ',dictOfQuery[i])\n",
    "#     print('=======================')\n",
    "    \n",
    "    file.close()\n",
    "    \n",
    "    dictOfRank = computeRank(dictOfQuery, listQuery)\n",
    "    \n",
    "    return dictOfRank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def addLineToDict(_dict, _line):\n",
    "    tempList = _line.split('\\t')\n",
    "    term = tempList[0]\n",
    "    \n",
    "    \n",
    "    termWeigth = tempList[1]\n",
    "    termWeigth = termWeigth.replace(';\\n','')\n",
    "    termWeigth = termWeigth.split(';')\n",
    "    listTermTemp = []\n",
    "    for i in termWeigth:\n",
    "        i = i.replace('[','')\n",
    "        i = i.replace(']','')\n",
    "        i = i.replace(' ','')\n",
    "        i = i.split(',')\n",
    "        i = [int(i[0]),float(i[1])]\n",
    "\n",
    "        listTermTemp.append(i)\n",
    "        \n",
    "    _dict[term] = listTermTemp\n",
    "    \n",
    "    return _dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def computeRank(_dictTerm, _query):\n",
    "    dictQuery = countQuery(_query)\n",
    "    dictSumOfSim = {}\n",
    "    dictSumWeigthOfSim = {}\n",
    "#     print(dictQuery)\n",
    "    for i in _dictTerm:\n",
    "        ni = len(_dictTerm[i])\n",
    "        countWq = countWeigth(ni, dictQuery[i])\n",
    "#         print(countWq)\n",
    "        for k in _dictTerm[i]:\n",
    "            if k[0] in dictSumOfSim:\n",
    "                dictSumOfSim[k[0]] += k[1]**2 \n",
    "            else:\n",
    "                dictSumOfSim[k[0]] = k[1]**2\n",
    "                \n",
    "            if k[0] in dictSumWeigthOfSim:\n",
    "                dictSumWeigthOfSim[k[0]] += countWq * (k[1]**2) \n",
    "            else:\n",
    "                dictSumWeigthOfSim[k[0]] = countWq * (k[1]**2)\n",
    "                \n",
    "#     print(dictSumOfSim)\n",
    "#     print(dictSumWeigthOfSim)\n",
    "                \n",
    "    dictSqrtOfSim = findSqrt(dictSumOfSim)\n",
    "    \n",
    "    dictOfRank = findRank(dictSqrtOfSim, dictSumWeigthOfSim)\n",
    "    \n",
    "    return dictOfRank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def countQuery(_query):\n",
    "    dictOfQuery = {}\n",
    "    for i in _query:\n",
    "        if i in dictOfQuery:\n",
    "            dictOfQuery[i] += 1\n",
    "        else:\n",
    "            dictOfQuery[i] = 1\n",
    "            \n",
    "    return dictOfQuery"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def findRank(_dictSqrt, _dictWeigth):\n",
    "#     print('sqrt : ',_dictSqrt)\n",
    "#     print('weigth : ',_dictWeigth)\n",
    "    dictOfRank = {}\n",
    "    for i in _dictSqrt:\n",
    "        dictOfRank[i] = (_dictWeigth[i]/_dictSqrt[i])\n",
    "        \n",
    "    return({k: v for k, v in sorted(dictOfRank.items(), key=lambda item: item[1], reverse=True)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "def findSqrt(_dict):\n",
    "    for i in _dict:\n",
    "        _dict[i] = math.sqrt(_dict[i])\n",
    "    \n",
    "    return _dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def countWeigth(_ni, _fij):\n",
    "    \n",
    "    file = open('sum_of_file.txt','r')\n",
    "    sumOfFile = int(file.read())\n",
    "    file.close()\n",
    "    \n",
    "    tf = computeTFLog(_fij)\n",
    "    idf = countIDF(sumOfFile, _ni)\n",
    "    weigth = tf * idf\n",
    "    weigth = weigth**2\n",
    "    \n",
    "    return weigth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "#tf = 1 + log(i,j)\n",
    "def computeTFLog(_fi):\n",
    "    return 1 + math.log(_fi,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "#idf = log(N/ni)\n",
    "def countIDF(_N, _ni):\n",
    "#     print('N : ',_N)\n",
    "#     print('_ni : ',_ni)\n",
    "    return math.log(1+(_N/_ni),2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "--- 0.023421764373779297 seconds ---\nRank :  {97: 510.75683569545686, 80: 395.175431406038, 77: 197.587715703019, 160: 197.587715703019, 195: 197.587715703019, 208: 197.587715703019, 215: 197.587715703019, 256: 197.587715703019, 488: 33.92396659928883}\n"
    }
   ],
   "source": [
    "import time\n",
    "start_time = time.time()\n",
    "rank = searchQuery('tf-idf.txt', 'Discount')\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))\n",
    "print('Rank : ', rank)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_list(rank, res):\n",
    "    res2 = int(res)\n",
    "    doc = []\n",
    "    score = []\n",
    "    doc2 = []\n",
    "    score2 = []\n",
    "    for x in rank:\n",
    "        doc.append(x)\n",
    "        score.append(rank[x])\n",
    "    if len(doc) > res2:\n",
    "        for i in range(0, res2):\n",
    "            doc2.append(doc[i])\n",
    "            score2.append(score[i])\n",
    "        return doc2, score2\n",
    "    else:\n",
    "        return doc, score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calRecall(hasil, kunci):\n",
    "    list_match = []\n",
    "    for i in range(0,len(kunci)):\n",
    "        for j in range(0,len(hasil)):\n",
    "            if kunci[i] == hasil[j]:\n",
    "                list_match.append(hasil[j])\n",
    "\n",
    "    recall = 100 * (len(list_match)/len(kunci))\n",
    "    return recall            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calPrecision(hasil, kunci):\n",
    "    list_match = []\n",
    "    for i in range(0,len(kunci)):\n",
    "        for j in range(0,len(hasil)):\n",
    "            if kunci[i] == hasil[j]:\n",
    "                list_match.append(hasil[j])\n",
    "\n",
    "    precision = 100 * (len(list_match)/len(hasil))\n",
    "    return precision"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>================</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "* Serving Flask app \"__main__\" (lazy loading)\n * Environment: production\n   WARNING: This is a development server. Do not use it in a production deployment.\n   Use a production WSGI server instead.\n * Debug mode: off\n * Running on http://127.0.0.1:5000/ (Press CTRL+C to quit)\n127.0.0.1 - - [10/May/2020 20:03:40] \"\u001b[37mPOST /search HTTP/1.1\u001b[0m\" 200 -\n(\"The House Ways and Means Committee\\nis moving toward passage of a trade bill that sponsors said was\\nintended to help open foreign markets to U.S. agricultural\\ngoods and to modify some U.S. agricultural trade laws.\\n    The trade subcommittee voted to require President Reagan to\\ntake into account the potential harm to U.S. agricultural\\nexports of any trade retaliation he might impose for foreign\\nunfair trade practices against other domestic industries.\\n    The bill would allow U.S. agricultural producers to seek\\ngovernment monitoring of imports if there is a reasonable\\nchance the industry would be harmed by an import surge.\\n    The full Ways and Means Committee is to consider the bill\\nnext week and congressional sources said they expect it will be\\napproved.\\n     In investigations involving a processed agricultural\\nproduct, trade associations of processors or producers would\\nhave to petition for relief from foreign dumping or unfair\\nduties.\\n     The bill sets out U.S. trade negotiating objectives for\\nthe Uruguay round of talks under the General Agreement on\\nTariffs and Trade. It would seek fair trade in agriculture,\\nseek to discipline restrictive or trade distorting import and\\nexport practices, to eliminate tariffs, subsidies, quotas and\\nnon-tariff barriers.\\n     President Reagan's authority to negotiate a new GATT\\nagreement would be extended through January 1993 and authority\\nto negotiate a free trade zone with Canada would be extended\\nthrough January 3, 1991.\\n    The bill extends Reagan's authority to negotiate an\\ninternational coffee agreement through October 31, 1989.\\n    It allows a refund of import duties paid on raw sugar\\nimported from November 1, 1977 to March 31, 1985 for production\\nof sugar or products containing sugar and destined for\\nre-export. The export of the sugar or products must occur\\nbefore Octoer 1, 1991.\\n    Presently, to qualify for the refund the sugar must be\\nprocessed within three years after import and exported within\\nfive years.\\n    Agriculture would also benefit from more rapid decisions in\\ncomplaints of unfair foreign trade practices or injury from\\nimports.\\n\", 'imports.\\n')\n('Blah blah blah.\\n', 'Blah blah blah.\\n')\n('The chairmen and senior Republican\\nmembers of the House and Senate tax writing committees proposed\\nlegislation to curb estate tax deduction on sales of stock to\\nan employee stock ownership plan.\\n    The proposal would raise federal revenues of 6.7 billion\\ndlrs over the fiscal year period 1987 to 1991.\\n    If adopted by Congress it would effect all transactions\\nafter Sept 26, 1987.\\n    The plan was proposed by House Ways and Means Committee\\nChairman Dan Rostenkowski (D-Ill), Rep John Duncan (R-Tenn),\\nSenate Finance Committee Chairman Lloyd Bentsen (D-Tex) and Sen\\nBob Packwood (R-Ore).\\n    In a statement Rostenkowski said the estate tax deduction\\nenacted last year as part of the tax reform bill was too broad\\nand would have cost the governmet seven billion dlrs over four\\nyears. The narrower deduction would cost the government less\\nthan 300 mln dlrs for the same years.\\n    He said it was designed to avoid sham transactions which\\nallowed estates to avoid taxes by transferring stock to ESOPs.\\n    Senate Finance Committee chairman Lloyd Bentsen said in a\\nstatement, \"The Tax Reform Act contains a provision that allows\\nmany wealthy individuals to avoid the federal estate tax\\nentirely when they die.\"\\n    He added, \"The provision was intended to encourage estates\\nto sell stock to employee stock ownership plans as a way of\\npromoting worker ownership; however, the provision was not\\nmeant to be broad enough to reduce federal revenues as much as\\nis currently estimated.\"\\n    He added, \"The bill I have introduced today calls for the\\nmodification of the provision in accordance with its intended\\npurpose.\"\\n', 'purpose.\"\\n')\n(\"The underwriters said the initial\\noffering of 833,334 Schult Homes Corp units is being made at\\nfive dlrs per unit.\\n    The underwriters, managed by Janney Montgomey Scott Inc and\\nWoolcott and Co Inc, said each unit consits of one common share\\nand one warrant to purchase one-half a common share at 5.50\\ndlrs per share until September one 1989 and thereafter at 6.50\\ndlrs per share until March 1991. The underwriters were granted\\nan over-allotment option of 125,000 units.\\n    They said the company will use its proceeds to pay a\\nportion of its subordinated note payable to Inland Steel Urban\\nDevelopment Corp issued in connection with the acquisition of\\nSchult from Inland. Based in Elkhart, Ind., Schult is the\\ncountry's oldest manufactured home producer.\\n\", \"country's oldest manufactured home producer.\\n\")\n('Schult Homes Corp announced an initial\\npublic offering of 833,334 units at five dlrs per unit, said\\nJanney Montgomery Scott Inc and Woolcott and Co, managing\\nunderwriters of the offering.\\n    They said each unit consists of one common share and one\\nwarrant to buy one-half share of common.\\n    The warrant will entitle holders to buy one-half common\\nshare at 5.50 dlrs per full share from March one, 1988, to\\nSeptember one, 1989, and thereafter at 6.50 dlrs per full share\\nuntil March 1991, they said.\\n', 'until March 1991, they said.\\n')\n('International Lease Finance Corp\\nsaid it picked the CFM International CFM56-5 high bypass\\nturbofan engine to power its three new Airbus Industries A320\\naircraft.\\n    International Lease said it is negotiating to buy up to 27\\nmore A320s. Initial aircraft deliveries are planned for 1991,\\nthe company said.\\n', 'the company said.\\n')\n('British Columbia\\nResources Investment Corp said it successfully concluded\\nrefinancing negotiations with bankers for a new 360 mln dlr\\nrestructured credit facility.\\n    The credit line will be in place for four years to March\\n31, 1991, but is extendable up to 10 years under certain\\ncircumstances which were not specified by the company.\\n    B.C. Resources said subsidiaries Westar Timber and Westar\\nPetroleum have settled revised lending agreements, but debt\\ndiscussions regarding subsidiary Westar Mining are continuing.\\n', 'discussions regarding subsidiary Westar Mining are continuing.\\n')\n('Noel Industries Inc said its board\\napproved in principle a private placement of 900 units, each\\nunit consisting of 1,000 dlrs of nine pct senior subordinated\\nconvertible debentures due Marcxh 31, 1991, and 95 warrants to\\npurchase Noel common.\\n    The company said chief executive officer Leon Ruchlamer has\\nsupplemented the planned funding with 300,000 dlrs.\\n    It said the investment package is subject to shareholder\\napproval and will be presented to its adjourned shareholder\\nmeeting on March 26.\\n    Noel said proceeds will be used for additional working\\ncapital and expanding its factory in Kingston, Jamaica.\\n    It said the debentures, which will be priced at 100 pct,\\nwill have interest payable semi-annually and be convertible\\ninto common after April 30, 1987, at seven dlrs a share.\\n    Each warrant will be exercisable after April 30 at 7.50\\ndlrs a share, the company added.\\n    It said holders of 80 pct of the units may request one\\nregistration by the company kof the underlying common shares\\nany time after Jan 15, 1988. Holders of the debentures and\\nwarrants will also have piggyback registration rights.\\n', 'warrants will also have piggyback registration rights.\\n')\n"
    }
   ],
   "source": [
    "ui = Flask(__name__)\n",
    "x = []\n",
    "@ui.route('/')\n",
    "def index():\n",
    "    return render_template('base.html')\n",
    "\n",
    "\n",
    "@ui.route('/search', methods=['POST', 'GET'])\n",
    "def search():\n",
    "    if request.method == 'POST':\n",
    "        list_title = []\n",
    "        list_date = []\n",
    "        list_body =[]\n",
    "        search = request.form[\"search\"]\n",
    "        method = request.form.get('met')\n",
    "        result = request.form.get('res')\n",
    "        if method == \"1\":\n",
    "            start_time = time.time()\n",
    "            hasil, score = search_query(search, result)\n",
    "        else :\n",
    "            start_time = time.time()\n",
    "            rank = searchQuery('tf-idf.txt', search)\n",
    "            hasil, score = to_list(rank, result)\n",
    "        for i in hasil:\n",
    "            title, date, body = get_atribut(i)\n",
    "            list_title.append(title)\n",
    "            list_date.append(date)\n",
    "            list_body.append(body)\n",
    "        search = pre_query(search)\n",
    "        search2 = ' '.join(map(str, search))\n",
    "        rec = calRecall(hasil, dict_jawaban.get(search2))\n",
    "        prec = calPrecision(hasil, dict_jawaban.get(search2))\n",
    "        ptime = (time.time() - start_time)\n",
    "        return render_template('respond.html', title = list_title, date = list_date,body=list_body, score = score, leng = len(list_title), time = ptime, recal = rec, precision = prec)\n",
    "    else:\n",
    "        return render_template('index.html')\n",
    "    \n",
    "if __name__ == \"__main__\":\n",
    "    ui.run()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.2 64-bit",
   "language": "python",
   "name": "python37264bit69774f291abd483f85df9cd74e3c9e37"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}