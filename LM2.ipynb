{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk.corpus import stopwords\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv_column_arr(data_frame, field, field_arr):\n",
    "    list_f_arr = []\n",
    "    dict_hasil = {}\n",
    "    f_arr = data_frame[field_arr]\n",
    "    for i in f_arr:\n",
    "        deleted_simbol = i.replace(\" \",\"\").replace(\"'\", \"\").replace(\"[\",\"\").replace(\"]\",\"\")\n",
    "        temp = deleted_simbol.split(\",\")\n",
    "        \n",
    "#         dict_hasil[term[i]] = num_doc\n",
    "        list_f_arr.append(temp)\n",
    "    dict_hasil = {field: data_frame[field], field_arr:list_f_arr }\n",
    "    df = pd.DataFrame(dict_hasil)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Baca File CSV yang Di Perlukan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "   doc_num                                              token\n0        1  [0.39, 1, 750, 1, 780, 1, 850, 1, 870, 1, 875,...\n1        2  [15:02:20.00, 26-feb-1987, 55, activ, also, am...\n2        3  [13.5, 15:03:27.51, 26-feb-1987, 31, 7.5, appl...\n3        4  [-1, -20, 1, 11, 12, 15, 15:07:13.72, 2, 2.70,...\n4        5  [-1986, 0.99, 1, 1.24, 1.35, 1.56, 1.65, 1.92,...",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>doc_num</th>\n      <th>token</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1</td>\n      <td>[0.39, 1, 750, 1, 780, 1, 850, 1, 870, 1, 875,...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2</td>\n      <td>[15:02:20.00, 26-feb-1987, 55, activ, also, am...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>3</td>\n      <td>[13.5, 15:03:27.51, 26-feb-1987, 31, 7.5, appl...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>4</td>\n      <td>[-1, -20, 1, 11, 12, 15, 15:07:13.72, 2, 2.70,...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>5</td>\n      <td>[-1986, 0.99, 1, 1.24, 1.35, 1.56, 1.65, 1.92,...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 3
    }
   ],
   "source": [
    "df_doc_num_token = pd.read_csv(\"doc_num_token.csv\")\n",
    "df_doc_num_token = conv_column_arr(df_doc_num_token,\"doc_num\",\"token\")\n",
    "df_doc_num_token.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "   term                                            doc_num\n0    -1  [0004, 0008, 0016, 0026, 0059, 0059, 0059, 005...\n1   -10                                             [0305]\n2  -100                                             [0427]\n3   -11                                             [0116]\n4  -110                                             [0427]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>term</th>\n      <th>doc_num</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>-1</td>\n      <td>[0004, 0008, 0016, 0026, 0059, 0059, 0059, 005...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>-10</td>\n      <td>[0305]</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>-100</td>\n      <td>[0427]</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>-11</td>\n      <td>[0116]</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>-110</td>\n      <td>[0427]</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 4
    }
   ],
   "source": [
    "df_token_doc_num = pd.read_csv(\"hasil_preprocessing.csv\")\n",
    "df_token_doc_num = conv_column_arr(df_token_doc_num, \"term\", \"doc_num\")\n",
    "df_token_doc_num.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "49088"
     },
     "metadata": {},
     "execution_count": 5
    }
   ],
   "source": [
    "# menghitung jml token\n",
    "jml_token = 0\n",
    "for i in range(len(df_doc_num_token)):\n",
    "    jml_token += len(df_doc_num_token[\"token\"].iloc[i])\n",
    "jml_token"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fungsi-fungsi untuk preprocessing query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_token(txt):\n",
    "    list_hasil = txt.split(\" \")            \n",
    "    return list_hasil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def removeNull(listOfWords): #['','','eat','food','','']\n",
    "    listOfWords = list(filter(None, listOfWords))\n",
    "    \n",
    "    return listOfWords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def caseFolding(listOfWords):\n",
    "    for i in range(len(listOfWords)):\n",
    "        listOfWords[i] = listOfWords[i].casefold()\n",
    "        \n",
    "    return listOfWords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def removeStopWord(listOfWords):\n",
    "    for i in listOfWords:\n",
    "        if i in stop_words:\n",
    "            listOfWords.remove(i)\n",
    "        \n",
    "    return listOfWords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stemming(listOfWords):\n",
    "    stemmer = SnowballStemmer(\"english\")\n",
    "    stemmed = [stemmer.stem(word) for word in listOfWords]\n",
    "    \n",
    "    return stemmed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fungsi utama main preprocessing query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pre_query(txt):\n",
    "    listOfWords = make_token(txt)\n",
    "    listOfWords = removeNull(listOfWords)\n",
    "    listOfWords = caseFolding(listOfWords)\n",
    "    listOfWords = removeStopWord(listOfWords)\n",
    "    listOfWords = stemming(listOfWords)\n",
    "    \n",
    "    return listOfWords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fungsi utama menghitung Score dokumen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hitung_score(list_word,a):\n",
    "    list_rank = []\n",
    "    list_doc_num2 = []\n",
    "    df_hasil = []\n",
    "    for i in range(len(df_doc_num_token)):\n",
    "        rank = 1\n",
    "        doc = df_doc_num_token.iloc[i]\n",
    "        for j in list_word:\n",
    "            count = doc[\"token\"].count(j)\n",
    "            panjang_doc = len(doc[\"token\"])\n",
    "            p_doc =  (count/panjang_doc) * a\n",
    "#           ----------------------------------\n",
    "            temp = df_token_doc_num[df_token_doc_num[\"term\"] == j]\n",
    "            if len(temp) == 0:\n",
    "                cout_m = 0\n",
    "            else: \n",
    "                count_m = len(temp.iloc[0][1])\n",
    "            p_m = (count_m/jml_token) * (1-a)\n",
    "            p = p_doc + p_m\n",
    "            rank *= p\n",
    "        list_rank.append(rank)\n",
    "        list_doc_num2.append(doc[\"doc_num\"])\n",
    "    dict_hasil = {\"doc_num\": list_doc_num2, \"rank\": list_rank}\n",
    "    df_hasil = pd.DataFrame(dict_hasil)\n",
    "    df_hasil = df_hasil.sort_values(by=[\"rank\"], ignore_index=True, ascending=False)\n",
    "    df_hasil = df_hasil[df_hasil[\"rank\"] != df_hasil.iloc[len(df_hasil)-1][\"rank\"]]\n",
    "    return df_hasil"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Coba query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "q1 = \"1991\"\n",
    "q2 = \"Discounts\"\n",
    "q3 = \"accessed 9-Mar-87\"\n",
    "q4 = \"Analyst appeared\"\n",
    "q5 = \"Analyst analyst appeared\"\n",
    "q6 = \"Chairman closed the last\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "'discount'"
     },
     "metadata": {},
     "execution_count": 15
    }
   ],
   "source": [
    "q = pre_query(q2)\n",
    "search2 = ' '.join(map(str, q))\n",
    "search2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "tags": [
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend"
    ]
   },
   "outputs": [],
   "source": [
    "df_hasil = hitung_score(q,0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_jawaban = {\n",
    "    \"1991\" : [31, 61, 70, 105, 190, 301, 422, 466],\n",
    "    \"discount\" : [77, 80, 97, 160, 195, 208, 215, 256, 488],\n",
    "    \"access 9-Mar-87\" : [32, 55, 110, 116, 134, 141, 142, 143, 144, 145, 146, 147, 148,\n",
    "                149, 150, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 261, \n",
    "                262, 263, 264, 265, 266, 267, 268, 269, 270, 321, 322, 323, 324, \n",
    "                325, 326, 327, 328, 329, 330, 348, 375, 381, 382, 383, 384, 385, \n",
    "                386, 387, 388, 389, 390, 441, 442, 443, 444, 445, 446, 447, 448,\n",
    "                449, 450],\n",
    "    \"analyst appear\" : [4, 8, 16, 18, 28, 45, 78, 95, 98, 111, 123, 133, 150, 180, 195, \n",
    "                235, 243, 262, 265, 292, 305, 320, 376, 399, 418, 459, 469, 493, \n",
    "                495, 496, 497],\n",
    "    \"analyst analyst appear\" : [4, 8, 16, 18, 28, 45, 78, 95, 98, 111, 123, 133, 150, 180, 195, \n",
    "                235, 243, 262, 265, 292, 305, 320, 376, 399, 418, 459, 469, 493, \n",
    "                495, 496, 497],\n",
    "    \"chairman close last\" : [1, 12, 18, 26, 28, 40, 43, 47, 52, 60, 61, 69, 78, 95, 98, 102, \n",
    "            107, 111, 114, 116, 133, 134, 135, 136, 138, 145, 148, 150, 153, \n",
    "            154, 155, 156, 161, 169, 174, 178, 180, 188, 194, 195, 197, 201, \n",
    "            203, 204, 207, 208, 214, 216, 224, 227, 230, 231, 232, 233, 234, \n",
    "            235, 242, 246, 254, 255, 256, 259, 261, 262, 263, 265, 267, 268, \n",
    "            269, 276, 279, 284, 288, 292, 303, 306, 314, 316, 318, 322, 324, \n",
    "            328, 330, 338, 339, 340, 343, 350, 355, 367, 371, 372, 375, 376, \n",
    "            378, 382, 383, 386, 392, 399, 404, 405, 409, 419, 420, 425, 430, \n",
    "            438, 439, 441, 444, 445, 446, 447, 448, 451, 454, 455, 457, 458, \n",
    "            463, 484, 488, 491, 492, 494, 495, 496, 498]\n",
    "}               "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "{'1991': [31, 61, 70, 105, 190, 301, 422, 466], 'discount': [77, 80, 97, 160, 195, 208, 215, 256, 488], 'access 9-Mar-87': [32, 55, 110, 116, 134, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 261, 262, 263, 264, 265, 266, 267, 268, 269, 270, 321, 322, 323, 324, 325, 326, 327, 328, 329, 330, 348, 375, 381, 382, 383, 384, 385, 386, 387, 388, 389, 390, 441, 442, 443, 444, 445, 446, 447, 448, 449, 450], 'analyst appear': [4, 8, 16, 18, 28, 45, 78, 95, 98, 111, 123, 133, 150, 180, 195, 235, 243, 262, 265, 292, 305, 320, 376, 399, 418, 459, 469, 493, 495, 496, 497], 'analyst analyst appear': [4, 8, 16, 18, 28, 45, 78, 95, 98, 111, 123, 133, 150, 180, 195, 235, 243, 262, 265, 292, 305, 320, 376, 399, 418, 459, 469, 493, 495, 496, 497], 'chairman close last': [1, 12, 18, 26, 28, 40, 43, 47, 52, 60, 61, 69, 78, 95, 98, 102, 107, 111, 114, 116, 133, 134, 135, 136, 138, 145, 148, 150, 153, 154, 155, 156, 161, 169, 174, 178, 180, 188, 194, 195, 197, 201, 203, 204, 207, 208, 214, 216, 224, 227, 230, 231, 232, 233, 234, 235, 242, 246, 254, 255, 256, 259, 261, 262, 263, 265, 267, 268, 269, 276, 279, 284, 288, 292, 303, 306, 314, 316, 318, 322, 324, 328, 330, 338, 339, 340, 343, 350, 355, 367, 371, 372, 375, 376, 378, 382, 383, 386, 392, 399, 404, 405, 409, 419, 420, 425, 430, 438, 439, 441, 444, 445, 446, 447, 448, 451, 454, 455, 457, 458, 463, 484, 488, 491, 492, 494, 495, 496, 498]}\n"
    }
   ],
   "source": [
    "print(dict_jawaban)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "[77, 80, 97, 160, 195, 208, 215, 256, 488]"
     },
     "metadata": {},
     "execution_count": 19
    }
   ],
   "source": [
    "get = dict_jawaban.get(search2)\n",
    "get"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# teratas = df_hasil.head(len(jawaban1)).sort_values(by=[\"doc_num\"], ignore_index=True, ascending=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# hasil_query = teratas[\"doc_num\"]\n",
    "# salah = []\n",
    "\n",
    "# for i in hasil_query:\n",
    "#     if int(i) not in jawaban1:\n",
    "#         salah.append(i)\n",
    "        \n",
    "# len(salah)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# teratas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from flask import Flask, request, render_template, url_for, redirect, jsonify"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_query(search, res):\n",
    "    list_hasil = []\n",
    "    list_score = []\n",
    "    res=int(res)\n",
    "    q = pre_query(search)\n",
    "    df_hasil = hitung_score(q,0.7)\n",
    "    if (res == 0) or (res > len(df_hasil)):\n",
    "        res2 = len(df_hasil)\n",
    "    else:\n",
    "        res2 = int(res)\n",
    "    for i in range(0, res2):\n",
    "        list_hasil.append(df_hasil.iloc[i][\"doc_num\"])\n",
    "        list_score.append(df_hasil.iloc[i][\"rank\"])\n",
    "    return list_hasil, list_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def readText(file):\n",
    "    words = []\n",
    "    f = open(file, 'r') #open file\n",
    "    text = f.read()    \n",
    "    f.close()\n",
    "\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_atribut(doc_id):\n",
    "    int_doc_id = int(doc_id)\n",
    "    str_doc_id = str(int_doc_id)\n",
    "    n = 4 - len(str_doc_id)\n",
    "    doc = (\"0\"*n) + str_doc_id\n",
    "    file_path = \"DataRouter\\\\Doc\" + doc + \".txt\"\n",
    "    \n",
    "    txt = readText(file_path)\n",
    "    \n",
    "    titlePattern = \"<TITLE>(.+)</TITLE>\"\n",
    "    title = re.findall(titlePattern, txt)[0]\n",
    "    \n",
    "    datePattern = \"<DATE>(.+)</DATE>\"\n",
    "    date = re.findall(datePattern, txt)[0]\n",
    "    \n",
    "    bodyPattern = \"<BODY>((.+\\s+)+)</BODY>\"\n",
    "    body = re.findall(bodyPattern, txt)[0]\n",
    "    print(body)\n",
    "    \n",
    "    return title, date, body"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>PUNYA BONTENG TF-IDF</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def searchQuery(_dir, _query):\n",
    "    file = open(_dir,'r')\n",
    "    \n",
    "    listQuery = pre_query(_query)\n",
    "    \n",
    "    dictOfQuery = {}\n",
    "    textPerLine = file.readlines()\n",
    "    for line in textPerLine:\n",
    "        temp = line.split('\\t')\n",
    "        if temp[0] in listQuery:\n",
    "            addLineToDict(dictOfQuery, line)\n",
    "            \n",
    "#     for i in dictOfQuery:\n",
    "#         print(i,' : ',dictOfQuery[i])\n",
    "#     print('=======================')\n",
    "    \n",
    "    file.close()\n",
    "    \n",
    "    dictOfRank = computeRank(dictOfQuery, listQuery)\n",
    "    \n",
    "    return dictOfRank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def addLineToDict(_dict, _line):\n",
    "    tempList = _line.split('\\t')\n",
    "    term = tempList[0]\n",
    "    \n",
    "    \n",
    "    termWeigth = tempList[1]\n",
    "    termWeigth = termWeigth.replace(';\\n','')\n",
    "    termWeigth = termWeigth.split(';')\n",
    "    listTermTemp = []\n",
    "    for i in termWeigth:\n",
    "        i = i.replace('[','')\n",
    "        i = i.replace(']','')\n",
    "        i = i.replace(' ','')\n",
    "        i = i.split(',')\n",
    "        i = [int(i[0]),float(i[1])]\n",
    "\n",
    "        listTermTemp.append(i)\n",
    "        \n",
    "    _dict[term] = listTermTemp\n",
    "    \n",
    "    return _dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def computeRank(_dictTerm, _query):\n",
    "    dictQuery = countQuery(_query)\n",
    "    dictSumOfSim = {}\n",
    "    dictSumWeigthOfSim = {}\n",
    "#     print(dictQuery)\n",
    "    for i in _dictTerm:\n",
    "        ni = len(_dictTerm[i])\n",
    "        countWq = countWeigth(ni, dictQuery[i])\n",
    "#         print(countWq)\n",
    "        for k in _dictTerm[i]:\n",
    "            if k[0] in dictSumOfSim:\n",
    "                dictSumOfSim[k[0]] += k[1]**2 \n",
    "            else:\n",
    "                dictSumOfSim[k[0]] = k[1]**2\n",
    "                \n",
    "            if k[0] in dictSumWeigthOfSim:\n",
    "                dictSumWeigthOfSim[k[0]] += countWq * (k[1]**2) \n",
    "            else:\n",
    "                dictSumWeigthOfSim[k[0]] = countWq * (k[1]**2)\n",
    "                \n",
    "#     print(dictSumOfSim)\n",
    "#     print(dictSumWeigthOfSim)\n",
    "                \n",
    "    dictSqrtOfSim = findSqrt(dictSumOfSim)\n",
    "    \n",
    "    dictOfRank = findRank(dictSqrtOfSim, dictSumWeigthOfSim)\n",
    "    \n",
    "    return dictOfRank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def countQuery(_query):\n",
    "    dictOfQuery = {}\n",
    "    for i in _query:\n",
    "        if i in dictOfQuery:\n",
    "            dictOfQuery[i] += 1\n",
    "        else:\n",
    "            dictOfQuery[i] = 1\n",
    "            \n",
    "    return dictOfQuery"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def findRank(_dictSqrt, _dictWeigth):\n",
    "#     print('sqrt : ',_dictSqrt)\n",
    "#     print('weigth : ',_dictWeigth)\n",
    "    dictOfRank = {}\n",
    "    for i in _dictSqrt:\n",
    "        dictOfRank[i] = (_dictWeigth[i]/_dictSqrt[i])\n",
    "        \n",
    "    return({k: v for k, v in sorted(dictOfRank.items(), key=lambda item: item[1], reverse=True)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "def findSqrt(_dict):\n",
    "    for i in _dict:\n",
    "        _dict[i] = math.sqrt(_dict[i])\n",
    "    \n",
    "    return _dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def countWeigth(_ni, _fij):\n",
    "    \n",
    "    file = open('sum_of_file.txt','r')\n",
    "    sumOfFile = int(file.read())\n",
    "    file.close()\n",
    "    \n",
    "    tf = computeTFLog(_fij)\n",
    "    idf = countIDF(sumOfFile, _ni)\n",
    "    weigth = tf * idf\n",
    "    weigth = weigth**2\n",
    "    \n",
    "    return weigth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "#tf = 1 + log(i,j)\n",
    "def computeTFLog(_fi):\n",
    "    return 1 + math.log(_fi,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "#idf = log(N/ni)\n",
    "def countIDF(_N, _ni):\n",
    "#     print('N : ',_N)\n",
    "#     print('_ni : ',_ni)\n",
    "    return math.log(1+(_N/_ni),2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "--- 0.01820063591003418 seconds ---\nRank :  {97: 510.75683569545686, 80: 395.175431406038, 77: 197.587715703019, 160: 197.587715703019, 195: 197.587715703019, 208: 197.587715703019, 215: 197.587715703019, 256: 197.587715703019, 488: 33.92396659928883}\n"
    }
   ],
   "source": [
    "import time\n",
    "start_time = time.time()\n",
    "rank = searchQuery('tf-idf.txt', 'Discount')\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))\n",
    "print('Rank : ', rank)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_list(rank, res):\n",
    "    res2 = int(res)\n",
    "    doc = []\n",
    "    score = []\n",
    "    doc2 = []\n",
    "    score2 = []\n",
    "    print(rank)\n",
    "    for x in rank:\n",
    "        doc.append(x)\n",
    "        score.append(rank[x])\n",
    "    if res2 != 0:\n",
    "        if len(doc) < res2:\n",
    "            for i in range(0, len(doc)):\n",
    "                doc2.append(doc[i])\n",
    "                score2.append(score[i])\n",
    "            return doc2, score2\n",
    "    else:\n",
    "        return doc, score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calRecall(hasil, kunci):\n",
    "    list_match = []\n",
    "    for i in range(0,len(kunci)):\n",
    "        for j in range(0,len(hasil)):\n",
    "            if kunci[i] == hasil[j]:\n",
    "                list_match.append(hasil[j])\n",
    "\n",
    "    recall = 100 * (len(list_match)/len(kunci))\n",
    "    return recall            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calPrecision(hasil, kunci):\n",
    "    list_match = []\n",
    "    for i in range(0,len(kunci)):\n",
    "        for j in range(0,len(hasil)):\n",
    "            if kunci[i] == hasil[j]:\n",
    "                list_match.append(hasil[j])\n",
    "\n",
    "    precision = 100 * (len(list_match)/len(hasil))\n",
    "    return precision"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>================</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "tags": [
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend"
    ]
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "* Serving Flask app \"__main__\" (lazy loading)\n * Environment: production\n   WARNING: This is a development server. Do not use it in a production deployment.\n   Use a production WSGI server instead.\n * Debug mode: off\n * Running on http://127.0.0.1:5000/ (Press CTRL+C to quit)\n127.0.0.1 - - [12/May/2020 20:52:23] \"\u001b[37mGET /search HTTP/1.1\u001b[0m\" 200 -\n127.0.0.1 - - [12/May/2020 20:54:02] \"\u001b[37mPOST /search HTTP/1.1\u001b[0m\" 200 -\n(\"Pharmacia AB said it was launching a\\n200 mln dlr Eurocommercial paper programme as part of a move to\\ninternationalise the company's financing.\\n    Market makers will be Credit Suisse First Boston, Morgan\\nStanley International and Svenska Handelsbanken PLC.\\n    Pharmacia treasurer Bertil Tiusanen said gaining direct\\naccess to the short term international capital market would\\nimprove its ability to meet its dollar borrowing requirement.\\n    He said it was a natural step for an internationally known\\ncompany whose shares are noted in Tokyo and Stockholm and are\\ntraded over the counter in the United States.\\n\", 'traded over the counter in the United States.\\n')\n(\"Pharmacia AB said it was launching a\\n200 mln dlr Eurocommercial paper programme as part of a move to\\ninternationalise the company's financing.\\n    Market makers will be Credit Suisse First Boston, Morgan\\nStanley International and Svenska Handelsbanken PLC.\\n    Pharmacia treasurer Bertil Tiusanen said gaining direct\\naccess to the short term international capital market would\\nimprove its ability to meet its dollar borrowing requirement.\\n    He said it was a natural step for an internationally known\\ncompany whose shares are noted in Tokyo and Stockholm and are\\ntraded over the counter in the United States.\\n\", 'traded over the counter in the United States.\\n')\n('Sens. Alan Cranston (D-Cal.) and\\nDaniel Evans (R-Wash.) said they introduced export licensing\\nreform legislation that could save U.S. companies hundreds of\\nthousands of dollars annually.\\n    \"Our emphasis is two-fold: Decontrol and de-license items\\nwhere such actions will not endanger our national security, and\\neliminate the Department of Defense\\'s de facto veto authority\\nover the licensing process,\" Cranston said.\\n    \"Our reforms should reduce licensing requirements by 65  to\\n70 pct,\" he told reporters. \"I am convinced that a more\\nrational...licensing process will boost exports.\"\\n    U.S. export controls are intended to deny Eastern bloc\\ncountries access to technology that could further their\\nmilitary capabilities.\\n    \"By refocusing our control resources on higher levels of\\ntechnology, technology that is truly critical, we will do a\\nbetter job of preventing diversion of critical technology to\\nour adversaries while promoting more exports,\" Cranston said.\\n    \"We cannot expect to continue to play a leading role in new\\ntechnology development in the future if we unduly restrict the\\nactivities of U.S. firms in the world market-place,\" Evans told\\nreporters.\\n', 'reporters.\\n')\n('Sens. Alan Cranston (D-Cal.) and\\nDaniel Evans (R-Wash.) said they introduced export licensing\\nreform legislation that could save U.S. companies hundreds of\\nthousands of dollars annually.\\n    \"Our emphasis is two-fold: Decontrol and de-license items\\nwhere such actions will not endanger our national security, and\\neliminate the Department of Defense\\'s de facto veto authority\\nover the licensing process,\" Cranston said.\\n    \"Our reforms should reduce licensing requirements by 65  to\\n70 pct,\" he told reporters. \"I am convinced that a more\\nrational...licensing process will boost exports.\"\\n    U.S. export controls are intended to deny Eastern bloc\\ncountries access to technology that could further their\\nmilitary capabilities.\\n    \"By refocusing our control resources on higher levels of\\ntechnology, technology that is truly critical, we will do a\\nbetter job of preventing diversion of critical technology to\\nour adversaries while promoting more exports,\" Cranston said.\\n    \"We cannot expect to continue to play a leading role in new\\ntechnology development in the future if we unduly restrict the\\nactivities of U.S. firms in the world market-place,\" Evans told\\nreporters.\\n', 'reporters.\\n')\n('Taiwan said it would soon relax import\\ncontrols on some 400 foreign items, including stationery and\\nbooks, in a further effort to allow trading partners,\\nespecially the U.S., Greater access to its markets.\\n    Taiwan announced the easing of import curbs on some 600\\nfarm and industrial products last month, a Council for Economic\\nPlanning and Development spokesman told Reuters.\\n    He said the new move was intended to balance trade between\\nTaiwan and its trading partners. The island\\'s trade surplus\\nreached a record 15.6 billion U.S. Dlrs last year, up from\\n10.62 billion in 1985.\\n    In January, Taiwan cut import tariffs on some 1,700 foreign\\nproducts and allowed imports of U.S. Wine, beer and cigarettes.\\n    \"We hope the measures will help reduce our trade surplus\\nthis year, especially with that of the U.S.,\" the spokesman\\nsaid.\\n    Washington is pressing Taiwan to open its markets wider as\\na way of cutting its trade deficit with the island, which rose\\nto 2.35 billion U.S. Dlrs in the first two months of 1987 from\\n1.87 billion in the year-earlier period.\\n', '1.87 billion in the year-earlier period.\\n')\n('The Reagan administration,\\nresponding to last year\\'s United Nations special session on\\nAfrica, today outlined a U.S. \"action program\" for sub-Saharan\\nAfrica focusing heavily on economic reform and self-help.\\n    A White House statement announced establishment of \"a\\nlong-term U.S. goal for all U.S. economic programs and policies\\nin sub-Saharan Africa: to end hunger in the region through\\neconomic growth, policy reform and private sector development.\"\\n    The statement said the \"program of action\" was recommended by\\na White House task force set up last September.\\n    In a series of recommendations, the task force called for\\nnew efforts to address Africa\\'s heavy debt burden and said U.S.\\nfood aid should stress production incentives to reinforce\\nAfrican nations\\' economic reform and productivity.\\n    It also said better African access to world markets should\\nbe promoted to reward good performance and enable African\\nnations to earn their way toward economic growth.\\n    The U.S. private sector should be mobilized to provide\\n\"private, voluntary and corporate involvement of a humanitarian\\n    It said donor countries \"should negotiate, through the\\nexisting International Monetary Fund/World Bank coordination\\nprocess, framework agreements with each sub-Saharan African\\ncountry to establish long-term structural adjustment and reform\\nprograms.\"\\n    The task force called for a separate budget account for\\nU.S. bilateral aid \"in order to focus better on rewarding\\neconomic performance and increasing the flexibility of U.S.\\nassistance programs for incentive economic reforms and private\\nsector development.\"\\n', 'sector development.\"\\n')\n('Eastman Kodak Co said it is introducing\\nfour information technology systems that will be led by today\\'s\\nhighest-capacity system for data storage and retrieval.\\n    The company said information management products will be\\nthe focus of a multi-mln dlr business-to-business\\ncommunications campaign under the threme \"The New Vision of\\nKodak.\"\\n    Noting that it is well-known as a photographic company,\\nKodak said its information technology sales exceeded four\\nbillion dlrs in 1986. \"If the Kodak divisions generating those\\nsales were independent, that company would rank among the top\\n100 of the Fortune 500,\" it pointed out.\\n    The objective of Kodak\\'s \"new vision\" communications\\ncampaign, it added, is to inform others of the company\\'s\\ncommitment to the business and industrial sector.\\n    Kodak said the campaign will focus in part on the\\ninformation management systems unveilded today --\\n    -- The Kodak optical disk system 6800 which can store more\\nthan a terabyte of information (a tillion bytes).\\n    - The Kodak KIMS system 5000, a networked information\\nmanagement system using optical disks or microfilm or both.\\n    -- The Kodak KIMS system 3000, an optical-disk-based system\\nthat allows users to integrate optical disks into their current\\ninformation management systems.\\n    -- The Kodak KIMS system 4500, a microfilm-based,\\ncomputer-assisted system which can be a starter system.\\n    Kodak said the optical disy system 6800 is a\\nwrite-once/ready-many-times type its Mass Memory Division will\\nmarket on a limited basis later this year and in quantity in\\n1988.\\n    Each system 6800 automated disk library can accommodate up\\nto 150, 14-inch optical disks. Each disk provides 6.8 gigabytes\\nof randomly accessible on-line storage. Thus, Kodak pointed\\nout, 150 disks render the more-than-a-terabyte capacity.\\n    Kodak said it will begin deliveries of the KIMS system 5000\\nin mid-1987. The open-ended and media-independent system \\nallows users to incorporate existing and emerging technologies,\\nincluding erasable optical disks, high-density magnetic media,\\nfiber optics and even artificial intelligence, is expected to\\nsell in the 700,000 dlr range.\\n    Initially this system will come in a 12-inch optical disk\\nversion which provides data storage and retrieval through a\\ndisk library with a capacity of up to 121 disks, each storing\\n2.6 gigabytes.\\n    Kodak said the KIMS system 3000 is the baseline member of\\nthe family of KIMS systems. Using one or two 12-inch manually\\nloaded optical disk drives, it will sell for about 150,000 dlrs\\nwith deliveries beginning in mid-year.\\n    The company said the system 3000 is fulling compatibal with\\nthe more powerful KIMS system 5000.\\n    It said the KIMS system 4500 uses the same hardware and\\nsoftware as the system 5000. It will be available in mid-1987\\nand sell in the 150,000 dlr range.\\n', 'and sell in the 150,000 dlr range.\\n')\n('Ground-breaking new systems for storing\\nand retrieving information are ushering in a new era for\\ncomputer companies and computer users.\\n    Within the past few weeks, International Business Machines\\nCorp &lt;IBM>, Eastman Kodak Co &lt;EK> and others have launched\\nproducts that radically increase the amount of data that can be\\ncatalogued and shelved in computerized libraries.\\n    \"This flurry of new technology could yield systems that\\nhandle a multimedia blitz of data,\" said Ian Warhaftig, a\\nsenior analyst with International Data Corp, Framingham, Mass.\\n    \"We\\'re developing new systems because our customers are\\nasking for them,\" Peter Giles, vice president and general\\nmanager of Kodak\\'s mass memory division, said in a recent\\ninterview.\\n    This demand is expected to soar in coming years. While\\nestimates vary, industry analysts project that providing\\nproducts and services geared for information storage and\\nretrieval could become a 20 billion dlr a year business by\\n1995.\\n    A wide range of technologies will be needed to meet the\\nvarying requirements of users.\\n    For example, a large credit verification service would want\\na system from which it could quickly retrieve credit data and\\nrelay it to its clients. A law firm, however, may need a\\ncomputerized law library in which capacity, rather than speed,\\nis the key feature. For architects and engineers, the ability\\nto store photographs, sketches and other graphics would be\\ncrucial.\\n    Regardless of the specific application, the trend is toward\\nconverting information - documents, video or film or even sound\\nrecordings - into to the digital language of zeros and ones\\nunderstood by computers.\\n    Saving space is the key goal in digitizing data for\\nstorage. An optical disk the size of a standard compact disk\\ncan store 550 megabytes of data, or about 250,000 pages of\\ntypewritten text.\\n    For this reason, the compact disk read-only memory, or\\nCD-ROM, is already a popular data storage media. Last week,\\nMicrosoft introduced Microsoft Bookshelf, a 300 dlr program\\nthat contains, on a single CD-ROM disk, a dictionary,\\nthesaurus, national ZIP code directory, Bartlett\\'s Familiar\\nQuotations, the World Almanac and other reference works.\\n    Scores of such products are already on the market, but most\\nare specialty items, such as Lotus Development Corp\\'s &lt;LOTS>\\nCD-ROM data base of stock information for financial analysts\\nand investors. \"Microsoft Bookshelf is important because it\\nmarks the arrival of CD-ROM packages for the general public,\"\\nsaid Ian Warhaftig of International Data Corp.\\n    One drawback of the CD-ROM, which uses a laser to record\\nand read data, is that that it requires a special player.\\nCD-ROM players for the retail market will appear later this\\nyear.\\n    Moreover, IDC\\'s Warhaftig said CD-ROM\\'s will be integrated\\nwith personal computers.\\n    \"Eventually, CD-ROM\\'s will fit right inside the PC box,\" he\\nsaid. \"Imagine the advantage of having a spelling checker and\\nthesaurus at your fingertips when you\\'re writing with a word\\nprocessing program.\"\\n    But CD-ROM\\'s are just the beginning. Also last week, Kodak\\nunveiled several systems that use 12-inch optical disks. The\\nlargest Kodak system uses a jukebox-like cabinet to hold up to\\n150 optical disks from which data can be retrieved in a matter\\nof seconds.\\n    Kodak also announced a 14-inch optical disk with 6.8\\ngigabytes of memory, five times the memory of a CD-ROM. The\\nKodak disk, which will not be available until the middle of\\n1988, is designed for users who need quick access to very large\\namounts of data, said Kodak\\'s Giles.\\n    Meanwhile, N.V. Philips &lt;PGLO.AS>, the Dutch electronics\\ngiant, is preparing to take optical disk technology a step\\nfurther with the first disk that can combine text, video and\\nsound. Philips said the system, called Called CD-Interactive,\\nwill be ready next year. It will include a new kind of CD-ROM\\nplayer that can hook up with a television set and stereo.\\n     Additional breakthroughs are expected as the next\\ngeneration of computer memory chips are introduced. Last month\\nIBM said it has made a four-megabyte chip, capable of storing\\nmore data than eight CD-ROM\\'s. Meantime, &lt;Nippon Telegraph and\\nTelephone> of Japan said it has built a 16-megabyte chip.\\n    Analysts say commercial versions of these chips are several\\nyears away, though some suspect that IBM may start volume\\nproduction of its four-megabyte chip sometime this year.\\n    Such chips will enable computer makers to build computers\\nwith immense memory capacities.\\n', 'with immense memory capacities.\\n')\n"
    }
   ],
   "source": [
    "ui = Flask(__name__)\n",
    "x = []\n",
    "@ui.route('/')\n",
    "def index():\n",
    "    return render_template('base.html')\n",
    "\n",
    "\n",
    "@ui.route('/search', methods=['POST', 'GET'])\n",
    "def search():\n",
    "    if request.method == 'POST':\n",
    "        list_title = []\n",
    "        list_date = []\n",
    "        list_body =[]\n",
    "        list_doc_number=[]\n",
    "        search = request.form[\"search\"]\n",
    "        method = request.form.get('met')\n",
    "        result = request.form.get('res')\n",
    "        if result == \"\":\n",
    "            result = 0 \n",
    "        if method == \"1\":\n",
    "            start_time = time.time()\n",
    "            hasil, score = search_query(search, result)\n",
    "            list_doc_number=hasil\n",
    "        else :\n",
    "            start_time = time.time()\n",
    "            rank = searchQuery('tf-idf.txt', search)\n",
    "            hasil, score = to_list(rank, result)\n",
    "            list_doc_number=hasil\n",
    "        for i in hasil:\n",
    "            title, date, body = get_atribut(i)\n",
    "            list_title.append(title)\n",
    "            list_date.append(date)\n",
    "            list_body.append(body)\n",
    "        search = pre_query(search)\n",
    "        search2 = ' '.join(map(str, search))\n",
    "        # rec = calRecall(hasil, dict_jawaban.get(search2))\n",
    "        # prec = calPrecision(hasil, dict_jawaban.get(search2))\n",
    "        ptime = (time.time() - start_time)\n",
    "        return render_template('respond.html', search=search,top=result,docnum=len(list_doc_number),doc=list_doc_number, title = list_title, date = list_date, body=list_body, score = score, leng = len(list_title), time = ptime)\n",
    "    else:\n",
    "        return render_template('index.html')\n",
    "    \n",
    "if __name__ == \"__main__\":\n",
    "    ui.run()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.2 64-bit",
   "language": "python",
   "name": "python37264bit69774f291abd483f85df9cd74e3c9e37"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}