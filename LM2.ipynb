{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk.corpus import stopwords\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv_column_arr(data_frame, field, field_arr):\n",
    "    list_f_arr = []\n",
    "    dict_hasil = {}\n",
    "    f_arr = data_frame[field_arr]\n",
    "    for i in f_arr:\n",
    "        deleted_simbol = i.replace(\" \",\"\").replace(\"'\", \"\").replace(\"[\",\"\").replace(\"]\",\"\")\n",
    "        temp = deleted_simbol.split(\",\")\n",
    "        \n",
    "#         dict_hasil[term[i]] = num_doc\n",
    "        list_f_arr.append(temp)\n",
    "    dict_hasil = {field: data_frame[field], field_arr:list_f_arr }\n",
    "    df = pd.DataFrame(dict_hasil)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Baca File CSV yang Di Perlukan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "   doc_num                                              token\n0        1  [0.39, 1, 750, 1, 780, 1, 850, 1, 870, 1, 875,...\n1        2  [15:02:20.00, 26-feb-1987, 55, activ, also, am...\n2        3  [13.5, 15:03:27.51, 26-feb-1987, 31, 7.5, appl...\n3        4  [-1, -20, 1, 11, 12, 15, 15:07:13.72, 2, 2.70,...\n4        5  [-1986, 0.99, 1, 1.24, 1.35, 1.56, 1.65, 1.92,...",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>doc_num</th>\n      <th>token</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1</td>\n      <td>[0.39, 1, 750, 1, 780, 1, 850, 1, 870, 1, 875,...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2</td>\n      <td>[15:02:20.00, 26-feb-1987, 55, activ, also, am...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>3</td>\n      <td>[13.5, 15:03:27.51, 26-feb-1987, 31, 7.5, appl...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>4</td>\n      <td>[-1, -20, 1, 11, 12, 15, 15:07:13.72, 2, 2.70,...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>5</td>\n      <td>[-1986, 0.99, 1, 1.24, 1.35, 1.56, 1.65, 1.92,...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 3
    }
   ],
   "source": [
    "df_doc_num_token = pd.read_csv(\"doc_num_token.csv\")\n",
    "df_doc_num_token = conv_column_arr(df_doc_num_token,\"doc_num\",\"token\")\n",
    "df_doc_num_token.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "   term                                            doc_num\n0    -1  [0004, 0008, 0016, 0026, 0059, 0059, 0059, 005...\n1   -10                                             [0305]\n2  -100                                             [0427]\n3   -11                                             [0116]\n4  -110                                             [0427]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>term</th>\n      <th>doc_num</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>-1</td>\n      <td>[0004, 0008, 0016, 0026, 0059, 0059, 0059, 005...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>-10</td>\n      <td>[0305]</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>-100</td>\n      <td>[0427]</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>-11</td>\n      <td>[0116]</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>-110</td>\n      <td>[0427]</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 4
    }
   ],
   "source": [
    "df_token_doc_num = pd.read_csv(\"hasil_preprocessing.csv\")\n",
    "df_token_doc_num = conv_column_arr(df_token_doc_num, \"term\", \"doc_num\")\n",
    "df_token_doc_num.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "49088"
     },
     "metadata": {},
     "execution_count": 5
    }
   ],
   "source": [
    "# menghitung jml token\n",
    "jml_token = 0\n",
    "for i in range(len(df_doc_num_token)):\n",
    "    jml_token += len(df_doc_num_token[\"token\"].iloc[i])\n",
    "jml_token"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fungsi-fungsi untuk preprocessing query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_token(txt):\n",
    "    list_hasil = txt.split(\" \")            \n",
    "    return list_hasil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def removeNull(listOfWords): #['','','eat','food','','']\n",
    "    listOfWords = list(filter(None, listOfWords))\n",
    "    \n",
    "    return listOfWords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def caseFolding(listOfWords):\n",
    "    for i in range(len(listOfWords)):\n",
    "        listOfWords[i] = listOfWords[i].casefold()\n",
    "        \n",
    "    return listOfWords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def removeStopWord(listOfWords):\n",
    "    for i in listOfWords:\n",
    "        if i in stop_words:\n",
    "            listOfWords.remove(i)\n",
    "        \n",
    "    return listOfWords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stemming(listOfWords):\n",
    "    stemmer = SnowballStemmer(\"english\")\n",
    "    stemmed = [stemmer.stem(word) for word in listOfWords]\n",
    "    \n",
    "    return stemmed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fungsi utama main preprocessing query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pre_query(txt):\n",
    "    listOfWords = make_token(txt)\n",
    "    listOfWords = removeNull(listOfWords)\n",
    "    listOfWords = caseFolding(listOfWords)\n",
    "    listOfWords = removeStopWord(listOfWords)\n",
    "    listOfWords = stemming(listOfWords)\n",
    "    \n",
    "    return listOfWords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fungsi utama menghitung Score dokumen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hitung_score(list_word,a):\n",
    "    list_rank = []\n",
    "    list_doc_num2 = []\n",
    "    df_hasil = []\n",
    "    for i in range(len(df_doc_num_token)):\n",
    "        rank = 1\n",
    "        doc = df_doc_num_token.iloc[i]\n",
    "        for j in list_word:\n",
    "            count = doc[\"token\"].count(j)\n",
    "            panjang_doc = len(doc[\"token\"])\n",
    "            p_doc =  (count/panjang_doc) * a\n",
    "#           ----------------------------------\n",
    "            temp = df_token_doc_num[df_token_doc_num[\"term\"] == j]\n",
    "            count_m = len(temp.iloc[0][1])\n",
    "            p_m = (count_m/jml_token) * (1-a)\n",
    "            p = p_doc + p_m\n",
    "            rank *= p\n",
    "        list_rank.append(rank)\n",
    "        list_doc_num2.append(doc[\"doc_num\"])\n",
    "    dict_hasil = {\"doc_num\": list_doc_num2, \"rank\": list_rank}\n",
    "    df_hasil = pd.DataFrame(dict_hasil)\n",
    "    df_hasil = df_hasil.sort_values(by=[\"rank\"], ignore_index=True, ascending=False)\n",
    "    df_hasil = df_hasil[df_hasil[\"rank\"] != df_hasil.iloc[len(df_hasil)-1][\"rank\"]]\n",
    "    return df_hasil"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Coba query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "q1 = \"1991\"\n",
    "q2 = \"Discounts\"\n",
    "q3 = \"accessed 9-Mar-87\"\n",
    "q4 = \"Analyst appeared\"\n",
    "q5 = \"Analyst analyst appeared\"\n",
    "q6 = \"Chairman closed the last\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "'discount'"
     },
     "metadata": {},
     "execution_count": 15
    }
   ],
   "source": [
    "q = pre_query(q2)\n",
    "search2 = ' '.join(map(str, q))\n",
    "search2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_hasil = hitung_score(q,0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_jawaban = {\n",
    "    \"1991\" : [31, 61, 70, 105, 190, 301, 422, 466],\n",
    "    \"discount\" : [77, 80, 97, 160, 195, 208, 215, 256, 488],\n",
    "    \"access 9-Mar-87\" : [32, 55, 110, 116, 134, 141, 142, 143, 144, 145, 146, 147, 148,\n",
    "                149, 150, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 261, \n",
    "                262, 263, 264, 265, 266, 267, 268, 269, 270, 321, 322, 323, 324, \n",
    "                325, 326, 327, 328, 329, 330, 348, 375, 381, 382, 383, 384, 385, \n",
    "                386, 387, 388, 389, 390, 441, 442, 443, 444, 445, 446, 447, 448,\n",
    "                449, 450],\n",
    "    \"analyst appear\" : [4, 8, 16, 18, 28, 45, 78, 95, 98, 111, 123, 133, 150, 180, 195, \n",
    "                235, 243, 262, 265, 292, 305, 320, 376, 399, 418, 459, 469, 493, \n",
    "                495, 496, 497],\n",
    "    \"analyst analyst appear\" : [4, 8, 16, 18, 28, 45, 78, 95, 98, 111, 123, 133, 150, 180, 195, \n",
    "                235, 243, 262, 265, 292, 305, 320, 376, 399, 418, 459, 469, 493, \n",
    "                495, 496, 497],\n",
    "    \"chairman close last\" : [1, 12, 18, 26, 28, 40, 43, 47, 52, 60, 61, 69, 78, 95, 98, 102, \n",
    "            107, 111, 114, 116, 133, 134, 135, 136, 138, 145, 148, 150, 153, \n",
    "            154, 155, 156, 161, 169, 174, 178, 180, 188, 194, 195, 197, 201, \n",
    "            203, 204, 207, 208, 214, 216, 224, 227, 230, 231, 232, 233, 234, \n",
    "            235, 242, 246, 254, 255, 256, 259, 261, 262, 263, 265, 267, 268, \n",
    "            269, 276, 279, 284, 288, 292, 303, 306, 314, 316, 318, 322, 324, \n",
    "            328, 330, 338, 339, 340, 343, 350, 355, 367, 371, 372, 375, 376, \n",
    "            378, 382, 383, 386, 392, 399, 404, 405, 409, 419, 420, 425, 430, \n",
    "            438, 439, 441, 444, 445, 446, 447, 448, 451, 454, 455, 457, 458, \n",
    "            463, 484, 488, 491, 492, 494, 495, 496, 498]\n",
    "}               "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "{'1991': [31, 61, 70, 105, 190, 301, 422, 466], 'discount': [77, 80, 97, 160, 195, 208, 215, 256, 488], 'access 9-Mar-87': [32, 55, 110, 116, 134, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 261, 262, 263, 264, 265, 266, 267, 268, 269, 270, 321, 322, 323, 324, 325, 326, 327, 328, 329, 330, 348, 375, 381, 382, 383, 384, 385, 386, 387, 388, 389, 390, 441, 442, 443, 444, 445, 446, 447, 448, 449, 450], 'analyst appear': [4, 8, 16, 18, 28, 45, 78, 95, 98, 111, 123, 133, 150, 180, 195, 235, 243, 262, 265, 292, 305, 320, 376, 399, 418, 459, 469, 493, 495, 496, 497], 'analyst analyst appear': [4, 8, 16, 18, 28, 45, 78, 95, 98, 111, 123, 133, 150, 180, 195, 235, 243, 262, 265, 292, 305, 320, 376, 399, 418, 459, 469, 493, 495, 496, 497], 'chairman close last': [1, 12, 18, 26, 28, 40, 43, 47, 52, 60, 61, 69, 78, 95, 98, 102, 107, 111, 114, 116, 133, 134, 135, 136, 138, 145, 148, 150, 153, 154, 155, 156, 161, 169, 174, 178, 180, 188, 194, 195, 197, 201, 203, 204, 207, 208, 214, 216, 224, 227, 230, 231, 232, 233, 234, 235, 242, 246, 254, 255, 256, 259, 261, 262, 263, 265, 267, 268, 269, 276, 279, 284, 288, 292, 303, 306, 314, 316, 318, 322, 324, 328, 330, 338, 339, 340, 343, 350, 355, 367, 371, 372, 375, 376, 378, 382, 383, 386, 392, 399, 404, 405, 409, 419, 420, 425, 430, 438, 439, 441, 444, 445, 446, 447, 448, 451, 454, 455, 457, 458, 463, 484, 488, 491, 492, 494, 495, 496, 498]}\n"
    }
   ],
   "source": [
    "print(dict_jawaban)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "[77, 80, 97, 160, 195, 208, 215, 256, 488]"
     },
     "metadata": {},
     "execution_count": 19
    }
   ],
   "source": [
    "get = dict_jawaban.get(search2)\n",
    "get"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# teratas = df_hasil.head(len(jawaban1)).sort_values(by=[\"doc_num\"], ignore_index=True, ascending=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# hasil_query = teratas[\"doc_num\"]\n",
    "# salah = []\n",
    "\n",
    "# for i in hasil_query:\n",
    "#     if int(i) not in jawaban1:\n",
    "#         salah.append(i)\n",
    "        \n",
    "# len(salah)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# teratas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from flask import Flask, request, render_template, url_for, redirect, jsonify"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_query(search, res):\n",
    "    list_hasil = []\n",
    "    list_score = []\n",
    "    res=int(res)\n",
    "    q = pre_query(search)\n",
    "    df_hasil = hitung_score(q,0.7)\n",
    "    if (res == 0) or (res > len(df_hasil)):\n",
    "        res2 = len(df_hasil)\n",
    "    else:\n",
    "        res2 = int(res)\n",
    "    for i in range(0, res2):\n",
    "        list_hasil.append(df_hasil.iloc[i][\"doc_num\"])\n",
    "        list_score.append(df_hasil.iloc[i][\"rank\"])\n",
    "    return list_hasil, list_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def readText(file):\n",
    "    words = []\n",
    "    f = open(file, 'r') #open file\n",
    "    text = f.read()    \n",
    "    f.close()\n",
    "\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_atribut(doc_id):\n",
    "    int_doc_id = int(doc_id)\n",
    "    str_doc_id = str(int_doc_id)\n",
    "    n = 4 - len(str_doc_id)\n",
    "    doc = (\"0\"*n) + str_doc_id\n",
    "    file_path = \"DataRouter\\\\Doc\" + doc + \".txt\"\n",
    "    \n",
    "    txt = readText(file_path)\n",
    "    \n",
    "    titlePattern = \"<TITLE>(.+)</TITLE>\"\n",
    "    title = re.findall(titlePattern, txt)[0]\n",
    "    \n",
    "    datePattern = \"<DATE>(.+)</DATE>\"\n",
    "    date = re.findall(datePattern, txt)[0]\n",
    "    \n",
    "    bodyPattern = \"<BODY>((.+\\s+)+)</BODY>\"\n",
    "    body = re.findall(bodyPattern, txt)[0]\n",
    "    print(body)\n",
    "    \n",
    "    return title, date, body"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>PUNYA BONTENG TF-IDF</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def searchQuery(_dir, _query):\n",
    "    file = open(_dir,'r')\n",
    "    \n",
    "    listQuery = pre_query(_query)\n",
    "    \n",
    "    dictOfQuery = {}\n",
    "    textPerLine = file.readlines()\n",
    "    for line in textPerLine:\n",
    "        temp = line.split('\\t')\n",
    "        if temp[0] in listQuery:\n",
    "            addLineToDict(dictOfQuery, line)\n",
    "            \n",
    "#     for i in dictOfQuery:\n",
    "#         print(i,' : ',dictOfQuery[i])\n",
    "#     print('=======================')\n",
    "    \n",
    "    file.close()\n",
    "    \n",
    "    dictOfRank = computeRank(dictOfQuery, listQuery)\n",
    "    \n",
    "    return dictOfRank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def addLineToDict(_dict, _line):\n",
    "    tempList = _line.split('\\t')\n",
    "    term = tempList[0]\n",
    "    \n",
    "    \n",
    "    termWeigth = tempList[1]\n",
    "    termWeigth = termWeigth.replace(';\\n','')\n",
    "    termWeigth = termWeigth.split(';')\n",
    "    listTermTemp = []\n",
    "    for i in termWeigth:\n",
    "        i = i.replace('[','')\n",
    "        i = i.replace(']','')\n",
    "        i = i.replace(' ','')\n",
    "        i = i.split(',')\n",
    "        i = [int(i[0]),float(i[1])]\n",
    "\n",
    "        listTermTemp.append(i)\n",
    "        \n",
    "    _dict[term] = listTermTemp\n",
    "    \n",
    "    return _dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def computeRank(_dictTerm, _query):\n",
    "    dictQuery = countQuery(_query)\n",
    "    dictSumOfSim = {}\n",
    "    dictSumWeigthOfSim = {}\n",
    "#     print(dictQuery)\n",
    "    for i in _dictTerm:\n",
    "        ni = len(_dictTerm[i])\n",
    "        countWq = countWeigth(ni, dictQuery[i])\n",
    "#         print(countWq)\n",
    "        for k in _dictTerm[i]:\n",
    "            if k[0] in dictSumOfSim:\n",
    "                dictSumOfSim[k[0]] += k[1]**2 \n",
    "            else:\n",
    "                dictSumOfSim[k[0]] = k[1]**2\n",
    "                \n",
    "            if k[0] in dictSumWeigthOfSim:\n",
    "                dictSumWeigthOfSim[k[0]] += countWq * (k[1]**2) \n",
    "            else:\n",
    "                dictSumWeigthOfSim[k[0]] = countWq * (k[1]**2)\n",
    "                \n",
    "#     print(dictSumOfSim)\n",
    "#     print(dictSumWeigthOfSim)\n",
    "                \n",
    "    dictSqrtOfSim = findSqrt(dictSumOfSim)\n",
    "    \n",
    "    dictOfRank = findRank(dictSqrtOfSim, dictSumWeigthOfSim)\n",
    "    \n",
    "    return dictOfRank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def countQuery(_query):\n",
    "    dictOfQuery = {}\n",
    "    for i in _query:\n",
    "        if i in dictOfQuery:\n",
    "            dictOfQuery[i] += 1\n",
    "        else:\n",
    "            dictOfQuery[i] = 1\n",
    "            \n",
    "    return dictOfQuery"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def findRank(_dictSqrt, _dictWeigth):\n",
    "#     print('sqrt : ',_dictSqrt)\n",
    "#     print('weigth : ',_dictWeigth)\n",
    "    dictOfRank = {}\n",
    "    for i in _dictSqrt:\n",
    "        dictOfRank[i] = (_dictWeigth[i]/_dictSqrt[i])\n",
    "        \n",
    "    return({k: v for k, v in sorted(dictOfRank.items(), key=lambda item: item[1], reverse=True)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "def findSqrt(_dict):\n",
    "    for i in _dict:\n",
    "        _dict[i] = math.sqrt(_dict[i])\n",
    "    \n",
    "    return _dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def countWeigth(_ni, _fij):\n",
    "    \n",
    "    file = open('sum_of_file.txt','r')\n",
    "    sumOfFile = int(file.read())\n",
    "    file.close()\n",
    "    \n",
    "    tf = computeTFLog(_fij)\n",
    "    idf = countIDF(sumOfFile, _ni)\n",
    "    weigth = tf * idf\n",
    "    weigth = weigth**2\n",
    "    \n",
    "    return weigth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "#tf = 1 + log(i,j)\n",
    "def computeTFLog(_fi):\n",
    "    return 1 + math.log(_fi,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "#idf = log(N/ni)\n",
    "def countIDF(_N, _ni):\n",
    "#     print('N : ',_N)\n",
    "#     print('_ni : ',_ni)\n",
    "    return math.log(1+(_N/_ni),2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "--- 0.02094411849975586 seconds ---\nRank :  {97: 510.75683569545686, 80: 395.175431406038, 77: 197.587715703019, 160: 197.587715703019, 195: 197.587715703019, 208: 197.587715703019, 215: 197.587715703019, 256: 197.587715703019, 488: 33.92396659928883}\n"
    }
   ],
   "source": [
    "import time\n",
    "start_time = time.time()\n",
    "rank = searchQuery('tf-idf.txt', 'Discount')\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))\n",
    "print('Rank : ', rank)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_list(rank, res):\n",
    "    res2 = int(res)\n",
    "    doc = []\n",
    "    score = []\n",
    "    doc2 = []\n",
    "    score2 = []\n",
    "    for x in rank:\n",
    "        doc.append(x)\n",
    "        score.append(rank[x])\n",
    "    if res2 != 0:\n",
    "        if len(doc) > res2:\n",
    "            for i in range(0, res2):\n",
    "                doc2.append(doc[i])\n",
    "                score2.append(score[i])\n",
    "            return doc2, score2\n",
    "    else:\n",
    "        return doc, score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_list2(rank, score, res):\n",
    "    res2 = int(res)\n",
    "    doc = []\n",
    "    doc2 = []\n",
    "    score2 = []\n",
    "    for x in rank:\n",
    "        doc.append(x)\n",
    "    if res2 != 0:\n",
    "        if len(doc) > res2:\n",
    "            for i in range(0, res2):\n",
    "                doc2.append(doc[i])\n",
    "                score2.append(score[i])\n",
    "            return doc2, score2\n",
    "    else:\n",
    "        return doc, score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calRecall(hasil, kunci):\n",
    "    list_match = []\n",
    "    for i in range(0,len(kunci)):\n",
    "        for j in range(0,len(hasil)):\n",
    "            if kunci[i] == hasil[j]:\n",
    "                list_match.append(hasil[j])\n",
    "\n",
    "    recall = 100 * (len(list_match)/len(kunci))\n",
    "    return recall            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calPrecision(hasil, kunci):\n",
    "    list_match = []\n",
    "    for i in range(0,len(kunci)):\n",
    "        for j in range(0,len(hasil)):\n",
    "            if kunci[i] == hasil[j]:\n",
    "                list_match.append(hasil[j])\n",
    "\n",
    "    precision = 100 * (len(list_match)/len(hasil))\n",
    "    return precision"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>================</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "tags": [
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend"
    ]
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "* Serving Flask app \"__main__\" (lazy loading)\n * Environment: production\n   WARNING: This is a development server. Do not use it in a production deployment.\n   Use a production WSGI server instead.\n * Debug mode: off\n * Running on http://127.0.0.1:5000/ (Press CTRL+C to quit)\n[2020-05-12 18:18:48,430] ERROR in app: Exception on /search [POST]\nTraceback (most recent call last):\n  File \"C:\\Users\\Asus\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\flask\\app.py\", line 2447, in wsgi_app\n    response = self.full_dispatch_request()\n  File \"C:\\Users\\Asus\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\flask\\app.py\", line 1952, in full_dispatch_request\n    rv = self.handle_user_exception(e)\n  File \"C:\\Users\\Asus\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\flask\\app.py\", line 1821, in handle_user_exception\n    reraise(exc_type, exc_value, tb)\n  File \"C:\\Users\\Asus\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\flask\\_compat.py\", line 39, in reraise\n    raise value\n  File \"C:\\Users\\Asus\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\flask\\app.py\", line 1950, in full_dispatch_request\n    rv = self.dispatch_request()\n  File \"C:\\Users\\Asus\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\flask\\app.py\", line 1936, in dispatch_request\n    return self.view_functions[rule.endpoint](**req.view_args)\n  File \"<ipython-input-41-a93cd4179127>\", line 20, in search\n    hasil, score = search_query(search, result)\n  File \"<ipython-input-24-ae252f8be3da>\", line 6, in search_query\n    if (res == 0) or (res > len(df_hasil)):\nTypeError: '>' not supported between instances of 'str' and 'int'\n127.0.0.1 - - [12/May/2020 18:18:48] \"\u001b[1m\u001b[35mPOST /search HTTP/1.1\u001b[0m\" 500 -\n"
    }
   ],
   "source": [
    "ui = Flask(__name__)\n",
    "x = []\n",
    "@ui.route('/')\n",
    "def index():\n",
    "    return render_template('base.html')\n",
    "\n",
    "\n",
    "@ui.route('/search', methods=['POST', 'GET'])\n",
    "def search():\n",
    "    if request.method == 'POST':\n",
    "        list_title = []\n",
    "        list_date = []\n",
    "        list_body =[]\n",
    "        list_doc_number=[]\n",
    "        search = request.form[\"search\"]\n",
    "        method = request.form.get('met')\n",
    "        result = request.form.get('res')\n",
    "        if method == \"1\":\n",
    "            start_time = time.time()\n",
    "            hasil, score = search_query(search, result)\n",
    "            list_doc_number=hasil\n",
    "        else :\n",
    "            start_time = time.time()\n",
    "            rank = searchQuery('tf-idf.txt', search)\n",
    "            hasil, score = to_list(rank, result)\n",
    "            list_doc_number=hasil)\n",
    "        for i in hasil:\n",
    "            title, date, body = get_atribut(i)\n",
    "            list_title.append(title)\n",
    "            list_date.append(date)\n",
    "            list_body.append(body)\n",
    "        search = pre_query(search)\n",
    "        search2 = ' '.join(map(str, search))\n",
    "        # rec = calRecall(hasil, dict_jawaban.get(search2))\n",
    "        # prec = calPrecision(hasil, dict_jawaban.get(search2))\n",
    "        ptime = (time.time() - start_time)\n",
    "        return render_template('respond.html', time=len(list_doc_number),doc=list_doc_number, title = list_title, date = list_date,body=list_body, score = score, leng = len(list_title), time = ptime)\n",
    "    else:\n",
    "        return render_template('index.html')\n",
    "    \n",
    "if __name__ == \"__main__\":\n",
    "    ui.run()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}